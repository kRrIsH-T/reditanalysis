{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from praw.models import MoreComments\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myReddit = praw.Reddit(\"bot1\")\n",
    "myReddit = praw.Reddit(client_id = \"_vdqTide0UmjdEmo2LA2aw\",client_secret=\"Ion6way_O-aUWngXlM0QHU2sa170Mg\",user_agent= \"hehe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subrr = myReddit.subreddit(\"AmItheAsshole\")\n",
    "\n",
    "fullText = \"\"\n",
    "for post in subrr.controversial(limit=10000):\n",
    "    body = post.selftext\n",
    "    body = \"\".join(body)\n",
    "    fullText += body\n",
    "for comment in post.comments:\n",
    "    if type(comment) == MoreComments:\n",
    "           continue\n",
    "    bod = comment.body\n",
    "    bod = \"\".join(bod)\n",
    "    fullText += bod\n",
    "textfile = open(\"D:\\prog\\hslab\\\\red.txt\",'w') \n",
    "\n",
    "fullText = ascii(fullText)\n",
    "\n",
    "textfile.write(fullText)\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"red.txt\"\n",
    "infile = open(filename,'r',encoding='latin-1')\n",
    "text = infile.read()\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrefinedStr = re.sub(r'\\—|\\'|\\`|\\\"|\\||\\.|\\*|\\[|\\{|\\}|\\(|\\)|\\]|\\;|\\:|\\,|\\^|\\=|\\-|\\+|\\_|\\!|\\?|\\/|\\>|\\<|\\&|\\\\|\\#|\\n', r' ', text) # removing special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = unrefinedStr.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330297\n",
      "['We', 'have', 'been', 'planning', 'on', 'going', 'on', 'vacation', 'to', 'cuba', 'for', 'the', 'Christmas', 'holidays', 'for', 'a', 'week', 'It', 'u2019s', 'pretty']\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "# Print the first 20 tokens\n",
    "print(tokens[:20]) \n",
    "#the above statement is equivalent to print(tokens[0:20:1]) which is basically for (int i=0;i<20;i++) in C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [myWord.lower() for myWord in tokens]\n",
    "\n",
    "# The above statement is the 'pythonic way' of writing code and is equivalent to:\n",
    "# for (int i=0; i<len(tokens); i++)\n",
    "#     tokens[i] = to_lowerr(tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "{'too', 'shan', 'weren', 'down', 'that', 'off', 'any', 'both', \"should've\", 'her', \"wouldn't\", \"shouldn't\", 'no', 'myself', 'until', 'now', 'under', 're', 'she', 'been', 'theirs', 'by', 'below', 'o', \"aren't\", 'won', \"shan't\", 'their', 'with', \"haven't\", 'then', 'at', 'doing', \"you're\", 'a', 'after', \"it's\", 'over', 'they', 'whom', 'ain', 'ourselves', 'during', 'shouldn', 'there', 'he', 'being', 'just', 'to', 've', 'into', 'if', 'so', 'i', \"don't\", 'this', 'll', 'an', 'very', 'hasn', 'should', 'haven', 'did', 'am', 'of', 'itself', 'what', 'such', 'had', 'but', \"she's\", 'don', 'yourselves', \"couldn't\", \"hasn't\", 'which', 'were', 'him', 'few', 'aren', 'your', 'out', 'some', 't', 'nor', 'it', 'having', 'because', 'be', \"isn't\", 'couldn', 'why', \"doesn't\", 'them', 'have', 'where', 'before', 'when', 'again', 'in', 'doesn', 'own', 'each', 'isn', 's', 'up', 'most', 'herself', 'and', 'between', 'these', 'on', 'above', 'his', 'themselves', 'ours', \"didn't\", \"you've\", 'or', 'other', 'through', 'can', 'will', 'me', 'our', \"needn't\", 'its', 'are', 'all', 'we', 'wouldn', 'my', 'as', \"weren't\", 'here', 'how', 'those', 'ma', 'has', \"that'll\", 'only', 'mightn', \"you'd\", 'yourself', 'the', 'while', \"hadn't\", 'once', 'y', 'same', 'wasn', 'than', 'from', 'needn', 'further', \"mustn't\", 'was', 'not', 'do', \"you'll\", 'more', 'against', 'who', 'does', 'himself', 'is', 'd', \"wasn't\", 'you', 'yours', 'mustn', 'hers', \"mightn't\", 'about', 'm', \"won't\", 'for', 'hadn', 'didn'}\n"
     ]
    }
   ],
   "source": [
    "myStopWords = set(stopwords.words(\"english\"))\n",
    "print(len(myStopWords))\n",
    "print(myStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "customStop = [\"\\u2019\",\"u2019\",\"\\u2019t\",\"u2019t\",\"time\",\"say\",\"got\",\"ask\",\"realli\",\"even\",\"feel\",\"since\",\"thing\",\"day\",\"call\",\"need\",\"think\",\"work\",\"know\",\"make\",\"number\",\"said\",\"told\",\"want\",\"like\",\"get\",\"ask\",\"per\",\"would\",\"return\",\"year\", \"rate\", \"vol\",\"one\",\"sir\",\"people\",\"honourable\",\"countries\",\"india\",\"contri\"]\n",
    "\n",
    "# This following line appends our new set of stop words to our set of stopwords myStopWords\n",
    "myStopWords.update(customStop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n",
      "{'too', 'shan', 'weren', 'down', 'that', 'off', 'any', 'both', \"should've\", 'honourable', 'her', \"wouldn't\", \"shouldn't\", 'no', 'myself', 'until', 'now', 'under', 'vol', 're', 'she', 'been', 'theirs', 'by', 'below', 'o', \"aren't\", 'won', 'feel', 'sir', \"shan't\", 'u2019t', 'their', 'with', \"haven't\", 'then', 'at', 'doing', 'work', 'said', \"you're\", 'a', 'after', \"it's\", 'over', 'they', 'whom', 'ain', 'ourselves', 'during', 'shouldn', 'there', 'since', 'want', 'he', 'being', '’', 'even', 'just', 'to', 've', 'into', 'if', 'so', 'i', \"don't\", 'this', 'number', 'countries', 'll', 'an', 'very', 'hasn', 'should', 'haven', 'return', 'did', 'am', 'get', 'of', 'itself', 'what', 'such', 'thing', 'had', 'but', \"she's\", 'don', 'yourselves', \"couldn't\", 'time', \"hasn't\", 'which', 'were', 'him', 'few', 'aren', 'your', 'out', 'need', 'some', 't', 'nor', 'it', 'having', 'because', 'be', \"isn't\", 'couldn', 'why', \"doesn't\", 'them', 'have', 'ask', 'where', 'before', 'when', 'india', 'again', 'in', 'told', 'contri', 'doesn', 'own', 'each', 'isn', 'think', 's', 'up', 'know', 'most', 'one', 'herself', 'and', 'would', 'between', 'these', 'on', 'above', 'people', 'his', 'themselves', 'ours', \"didn't\", \"you've\", 'or', 'other', 'through', 'can', 'rate', 'will', 'me', 'our', \"needn't\", 'its', 'are', 'all', 'realli', 'day', 'we', 'wouldn', 'got', 'my', 'as', \"weren't\", 'here', 'how', 'make', 'those', 'ma', 'has', \"that'll\", 'only', 'mightn', \"you'd\", 'yourself', 'the', 'while', 'say', \"hadn't\", 'once', 'y', 'u2019', 'same', '’t', 'wasn', 'than', 'from', 'needn', 'further', \"mustn't\", 'was', 'not', 'do', \"you'll\", 'more', 'against', 'who', 'does', 'call', 'himself', 'is', 'd', 'per', \"wasn't\", 'you', 'yours', 'mustn', 'hers', \"mightn't\", 'about', 'm', 'like', 'year', \"won't\", 'for', 'hadn', 'didn'}\n"
     ]
    }
   ],
   "source": [
    "print(len(myStopWords))\n",
    "print(myStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bag = []\n",
    "\n",
    "# recall: \"for word in tokens\" does just as what is sounds in simple English. it goes through the array(list in python) and accesses word by word\n",
    "\n",
    "for word in tokens:\n",
    "    if word not in myStopWords and len(word)>2 and  word.isnumeric()==False:\n",
    "        Bag.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124286\n"
     ]
    }
   ],
   "source": [
    "print(len(Bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "SnowStem = SnowballStemmer('english')\n",
    "Lemmatiser = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "StemBag = []\n",
    "LemBag = []\n",
    "for word in Bag:\n",
    "    StemBag.append(SnowStem.stem(word))\n",
    "    LemBag.append(Lemmatiser.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyDict = {}\n",
    "for word in StemBag:\n",
    "    if word in MyDict.keys():\n",
    "        MyDict[word] += 1\n",
    "    else:\n",
    "        MyDict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedDict = sorted(MyDict.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sortedDict[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedDict = sorted(MyDict.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sortedDict[:100])\n",
    "\n",
    "keyArr = [item[0] for item in sortedDict[:10]]\n",
    "valArr = [item[1] for item in sortedDict[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width = 1000, height = 500).generate_from_frequencies(MyDict)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.bar(keyArr, valArr)\n",
    "\n",
    "# set the x and y axis\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# label for each bar\n",
    "plt.title('Bar graph of word frequencies')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
